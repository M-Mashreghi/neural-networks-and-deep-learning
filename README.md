# neural-networks-and-deep-learning

<h1>HW1</h1> <h1>Question 1</h1> <p> - The DFA state transition table was converted to a state description table suitable for implementing a neural network. The table specified the current state (0-3), input (0 or 1), next state (0-3), and whether the state is an accepting state (0 or 1). </p> <p> - Three separate networks were drawn to represent the transitions: <ul> <li>A 2-4-2 network for next state. Inputs are current state and input, outputs are next state.</li> <li>A 2-3-1 network for accepted state. Input is just current state, output is accept or not.</li> <li>A 3-4-3 combined network with inputs as current state, input and accepted state. Outputs are next state and accepted state.</li> </ul> </p> <p> - The networks were optimized to have the minimal neurons and threshold. The combined network with 3 inputs, 3 hidden neurons and threshold of 2 was optimal. </p> <p> - The combined network was implemented in Python. A loop tested all input combinations, printing the next state and accepted values. The results matched the DFA, validating the network works. </p> <h1>Question 2</h1> <p> - Two sample datasets were defined, each with 100 points: <ul> <li>Set 1 had x ~ N(0, 0.1) and y ~ N(0, 0.4)</li> <li>Set 2 had x ~ N(1, 0.2) and y ~ N(1, 0.2)</li> </ul> </p> <p> - The scatter plot showed two distinct clusters centered at (0,0) and (1,1). </p> <p> - An Adaline network was trained to separate the datasets. The mean squared error decreased smoothly, indicating good separation. </p> <p> - The variance of the datasets was then increased. The data overlapped more and the Adaline did not separate as cleanly. </p> <h1>Question 3</h1> <p> - The MNIST dataset was loaded using TensorFlow/Keras and some sample images visualized. </p> <p> - The data was normalized to the range [0,1]. The number of images per digit class was visualized. </p> <p> - An autoencoder was built with encoder and decoder components. It was trained and the loss curves showed steady decrease. </p> <p> - The 30-neuron encoder output was used as features for a simple 2 layer classifier. Accuracy and loss curves were plotted during training. </p> <p> - The classifier achieved 97% test accuracy. The confusion matrix showed some errors between similar digits like 4/9. </p> <h1>Question 4</h1> <p> - The car price dataset was loaded into Pandas and exploratory analysis done: <ul> <li>Checked and handled missing values</li> <li>Extracted company name from car names</li> <li>Converted categorical features to numerical</li> <li>Identified mileage as most correlated with price</li> <li>Plotted price distribution and vs mileage</li> </ul> </p> <p> - Data was split into train/test and normalized </p> <p> - 3 MLPs were built with varying layers and compared. 3 layers performed best with lowest validation loss. </p> <p> - The hyperparameters of the 3 layer MLP were tuned. Adam optimizer and MSE loss worked best. </p> <p> - Made price predictions on test set. Compared to true values to calculate error. </p>
