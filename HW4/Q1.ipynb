{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-10-23T09:42:10.516010Z",
     "iopub.status.busy": "2023-10-23T09:42:10.515630Z",
     "iopub.status.idle": "2023-10-23T09:42:11.383520Z",
     "shell.execute_reply": "2023-10-23T09:42:11.382706Z",
     "shell.execute_reply.started": "2023-10-23T09:42:10.515980Z"
    },
    "id": "d2hVROYvN-yW",
    "outputId": "22c8b400-3b05-4b2c-b37a-d8449fe6802b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 23 09:42:11 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   21C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsKM_aaY3QAb"
   },
   "source": [
    "# **Dataset Loading:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-23T09:42:11.386163Z",
     "iopub.status.busy": "2023-10-23T09:42:11.385469Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Using cached kaggle-1.5.16-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from kaggle) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: python-slugify in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from kaggle) (8.0.1)\n",
      "Requirement already satisfied: urllib3 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from kaggle) (1.26.15)\n",
      "Requirement already satisfied: bleach in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from kaggle) (6.0.0)\n",
      "Requirement already satisfied: webencodings in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from requests->kaggle) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from requests->kaggle) (3.4)\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.5.16\n",
      "flickr8k.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Archive:  flickr8k.zip\n",
      "replace flickr_data/Images/1000268201_693b08cb0e.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "!pip install kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!echo '{\"username\":\"mohammadmashreghi\",\"key\":\"80f77d32644c4e3051aec2d1d4fd7270\"}' > ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d adityajn105/flickr8k -p ~/workspace\n",
    "\n",
    "!unzip flickr8k.zip -d flickr_data\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0C0LBDgx9hMP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "# Prerequisites\n",
    "import os  # For operating system-related operations\n",
    "import torch  # For deep learning framework\n",
    "import torchvision.transforms as T  # For image transformations\n",
    "import matplotlib.pyplot as plt  # For visualizations\n",
    "import matplotlib.image as mpimg  # For working with images\n",
    "from collections import Counter  # For counting elements in a collection\n",
    "from torch.nn.utils.rnn import pad_sequence  # For padding sequences\n",
    "from torch.utils.data import DataLoader, Dataset  # For creating data loaders and datasets\n",
    "from nltk.tokenize import RegexpTokenizer  # For tokenizing text\n",
    "from PIL import Image  # For working with images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMJOmoFe9P-3"
   },
   "source": [
    "## **Data Retrieval:**\n",
    "Fetching file names for captions and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "YtcfTPpQGH-I",
    "outputId": "3e6f1a63-53e8-45a4-89bc-5e85eff28a41",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the path to the captions file\n",
    "captions_path = '/home/jovyan/workspace/flickr_data/captions.txt'\n",
    "\n",
    "# Load the captions file into a pandas DataFrame\n",
    "captions_df = pd.read_csv(captions_path)\n",
    "\n",
    "# Count the number of images with captions\n",
    "num_images = len(captions_df)\n",
    "\n",
    "# Print the total number of images with captions\n",
    "print(f\"Total number of images with captions: {num_images}\")\n",
    "\n",
    "# Print the number of unique images\n",
    "unique_images = len(captions_df['image'].unique())\n",
    "print(f\"Number of unique images with captions: {unique_images}\")\n",
    "\n",
    "# Print the number of unique captions\n",
    "unique_captions = len(captions_df['caption'].unique())\n",
    "print(f\"Number of unique captions: {unique_captions}\")\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "print(\"\\nSample captions:\")\n",
    "captions_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-FWDUzY9_Kd"
   },
   "source": [
    "## **Creating a Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wgtWa-FRLlFK",
    "outputId": "26e8e54e-4264-4f1d-dd30-5dce8aba8b37",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the Vocabulary class\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "class Vocabulary:\n",
    "  def __init__(self, frequency_threshold):\n",
    "    # Special tokens and their corresponding indices\n",
    "    self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "    self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "    self.frequency_threshold = frequency_threshold\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    # Return the total number of tokens in the vocabulary\n",
    "    return len(self.itos)\n",
    "\n",
    "  def tokenizer(self, text):\n",
    "    # Tokenize the text using a regular expression tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return [token.lower() for token in tokenizer.tokenize(text)]\n",
    "  \n",
    "  def build_vocab(self, caption_list):\n",
    "    frequencies = {}\n",
    "    idx = 4\n",
    "\n",
    "    # Iterate over each caption in the list\n",
    "    for caption in caption_list:\n",
    "      # Tokenize the caption\n",
    "      for token in self.tokenizer(caption):\n",
    "        # Update the token frequencies\n",
    "        if token not in frequencies:\n",
    "          frequencies[token] = 1\n",
    "        else:\n",
    "          frequencies[token] += 1\n",
    "        \n",
    "        # Check if the token frequency reaches the threshold\n",
    "        if frequencies[token] == self.frequency_threshold:\n",
    "          # Add the token to the vocabulary with a new index\n",
    "          self.stoi[token] = idx\n",
    "          self.itos[idx] = token\n",
    "          idx += 1\n",
    "    \n",
    "  def numericalize(self, text):\n",
    "    # Tokenize the text\n",
    "    tokenized_text = self.tokenizer(text)\n",
    "\n",
    "    # Convert tokens to their corresponding indices in the vocabulary\n",
    "    return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text]\n",
    "\n",
    "# Create an instance of the Vocabulary class with a frequency threshold of 1\n",
    "v = Vocabulary(frequency_threshold=1)\n",
    "\n",
    "# Build the vocabulary using a list of captions\n",
    "v.build_vocab([\"I am Mohammad Javad Ahmadi, a student of Dr. Keller's Deep Learning course.\"])\n",
    "\n",
    "# Print the vocabulary dictionary\n",
    "print(f\"Vocabulary dictionary: {v.stoi}\")\n",
    "\n",
    "# Numericalize a new text using the vocabulary\n",
    "numericalized_text = v.numericalize(\"I am Mohammad Javad Ahmadi, a student of Dr. Keller's Deep Learning course.\")\n",
    "\n",
    "# Print the numericalized text\n",
    "print(f\"Numericalized text: {numericalized_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBhCirf1-Fze"
   },
   "source": [
    "## **Creating a Custom Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjGWYvfbHVUY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "  def __init__(self, root_dir, caption_file, transform=None, frequency_threshold=5, data_type='train'):\n",
    "    # Read the caption file into a DataFrame\n",
    "    df = pd.read_csv(caption_file)\n",
    "    \n",
    "    # Split the dataset into train and test based on the data_type\n",
    "    if data_type == 'train':\n",
    "      # Select the first 90% of the DataFrame for training\n",
    "      self.df = df.iloc[:int(0.9 * len(df))]\n",
    "    elif data_type == 'test':\n",
    "      # Select the remaining 10% of the DataFrame for testing\n",
    "      test_start_index = int(0.9 * len(df))\n",
    "      self.df = df.iloc[test_start_index:].reset_index(drop=True)\n",
    "\n",
    "    else:\n",
    "      # If data_type is neither 'train' nor 'test', do nothing\n",
    "      pass\n",
    "\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "\n",
    "    # Store the image names and captions\n",
    "    self.imgs = self.df['image']\n",
    "    self.captions = self.df['caption']\n",
    "\n",
    "    # Initialize the vocabulary and build the vocabulary\n",
    "    self.vocab = Vocabulary(frequency_threshold)\n",
    "    self.vocab.build_vocab(self.captions.tolist())\n",
    "\n",
    "  def __len__(self):\n",
    "    # Return the length of the dataset\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # Get the caption and image information for the given index\n",
    "    caption = self.captions[index]\n",
    "    image_name = self.imgs[index]\n",
    "    image_path = self.root_dir + '/' + image_name\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    if self.transform is not None:\n",
    "      # Apply the specified transformations to the image\n",
    "      img = self.transform(img)\n",
    "\n",
    "    # Convert the caption to a vectorized form\n",
    "    vectorized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "    vectorized_caption += self.vocab.numericalize(caption)\n",
    "    vectorized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "    # Return the image and its vectorized caption as tensors\n",
    "    return img, torch.tensor(vectorized_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOxq3PPP-LTe"
   },
   "source": [
    "## **Make & Preprocess Dataset and Visualizing Data Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1AbDDhsTtMf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_image(image_tensor, title=None):\n",
    "    \"\"\"\n",
    "    Display an image represented as a tensor.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): The input image tensor.\n",
    "        title (str, optional): The title of the image. Defaults to None.\n",
    "    \"\"\"\n",
    "    # Convert the image tensor to a NumPy array and change the dimensions\n",
    "    image_np = image_tensor.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(image_np)\n",
    "    \n",
    "    # Set the title of the image if provided\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    \n",
    "    # Pause a bit to allow the plot to be updated\n",
    "    plt.pause(0.001)\n",
    "\n",
    "# Import the necessary libraries\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Define the transform to be applied to the images\n",
    "transforms = T.Compose([\n",
    "    T.Resize((224, 224)),  # Resize the images to (224, 224)\n",
    "    T.ToTensor()  # Convert the images to tensors\n",
    "])\n",
    "\n",
    "# Create instances of the FlickrDataset class for training and testing\n",
    "train_dataset = FlickrDataset(\n",
    "    root_dir=\"/home/jovyan/workspace/flickr_data/Images\",  # Path to the root directory of the images\n",
    "    caption_file=\"/home/jovyan/workspace/flickr_data/captions.txt\",  # Path to the captions file\n",
    "    transform=transforms,  # Apply the defined transforms to the images\n",
    "    data_type='train'  # Specify the data type as 'train'\n",
    ")\n",
    "\n",
    "test_dataset = FlickrDataset(\n",
    "    root_dir=\"/home/jovyan/workspace/flickr_data/Images\",  # Path to the root directory of the images\n",
    "    caption_file=\"/home/jovyan/workspace/flickr_data/captions.txt\",  # Path to the captions file\n",
    "    transform=transforms,  # Apply the defined transforms to the images\n",
    "    data_type='test'  # Specify the data type as 'test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "-lMa5tovYLey",
    "outputId": "ce6522ea-23ac-4b86-f0a5-422652df5dc2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Retrieve the image and captions for the first data point from the training dataset\n",
    "image, captions = train_dataset[100]\n",
    "\n",
    "# Create a new figure and plot the image\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0))\n",
    "\n",
    "# Set the title of the image\n",
    "ax.set_title(\"Image\")\n",
    "\n",
    "# Print the tokenized captions\n",
    "print(\"Tokenized Captions:\", captions)\n",
    "\n",
    "# Convert the tokenized captions to their corresponding words\n",
    "words = [train_dataset.vocab.itos[token] for token in captions.tolist()]\n",
    "\n",
    "# Print the sentence\n",
    "print(\"Sentence:\")\n",
    "print(words)\n",
    "\n",
    "# Save the figure as a PDF\n",
    "pdf = PdfPages(\"image100tr.pdf\")\n",
    "pdf.savefig(fig)\n",
    "pdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "caplZ7B3cRLQ",
    "outputId": "a2ff7224-1451-4723-85b2-285a95b82ca3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Retrieve the image and captions for the 100th data point from the training dataset\n",
    "image, captions = test_dataset[100]\n",
    "\n",
    "# Create a new figure and plot the image\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0))\n",
    "\n",
    "# Set the title of the image\n",
    "ax.set_title(\"Image\")\n",
    "\n",
    "# Print the tokenized captions\n",
    "print(\"Tokenized Captions:\", captions)\n",
    "\n",
    "# Convert the tokenized captions to their corresponding words\n",
    "words = [test_dataset.vocab.itos[token] for token in captions.tolist()]\n",
    "\n",
    "# Print the sentence\n",
    "print(\"Sentence:\")\n",
    "print(words)\n",
    "\n",
    "# Save the figure as a PDF\n",
    "pdf = PdfPages(\"image100ts.pdf\")\n",
    "pdf.savefig(fig)\n",
    "pdf.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZE2A7UT-30q"
   },
   "source": [
    "## **Implement Padding for Sentences in Each Batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQOmF0TmUWcu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Apppadd:\n",
    "\n",
    "    def __init__(self, pad_idx, batch_first=False):\n",
    "\n",
    "        self.pad_idx = pad_idx\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Extract images from the batch and unsqueeze them\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "\n",
    "        # Extract captions from the batch\n",
    "        captions = [item[1] for item in batch]\n",
    "\n",
    "        # Pad the captions using pad_sequence\n",
    "        captions = pad_sequence(captions, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
    "\n",
    "        return imgs, captions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijIyQdSX-3JS"
   },
   "source": [
    "## **Evaluating Dataloaders with Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Nevjpj8x10D",
    "outputId": "af3afa10-c947-49d4-cc5f-4ab0597bdd9e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "print(\"Maximum number of workers:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_Tm1SeRUYI_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 10\n",
    "\n",
    "# Token to represent the padding\n",
    "pad_idx = train_dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "# Create a DataLoader object to load the training dataset\n",
    "data_loader = DataLoader(\n",
    "    dataset=train_dataset,              # Specify the dataset to load\n",
    "    batch_size=BATCH_SIZE,              # Set the batch size\n",
    "    num_workers=NUM_WORKERS,            # Set the number of worker processes for data loading\n",
    "    shuffle=True,                       # Shuffle the data for each epoch\n",
    "    collate_fn=Apppadd(pad_idx=pad_idx, batch_first=True)   # Specify the collate function for padding captions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MiwlsHx-Z6fU",
    "outputId": "c20e842c-ad07-4fc7-fe08-009e6f339e4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generating the iterator from the dataloader\n",
    "dataiter = iter(data_loader)\n",
    "\n",
    "# Getting the next batch\n",
    "batch = next(dataiter)\n",
    "\n",
    "# Unpacking the batch\n",
    "images, captions = batch\n",
    "\n",
    "# Determine the effective batch size\n",
    "effective_batch_size = min(BATCH_SIZE, len(images))\n",
    "\n",
    "# Showing information of each image in the batch\n",
    "for i in range(effective_batch_size):\n",
    "    img, cap = images[i], captions[i]\n",
    "\n",
    "    # Extracting the caption label from the numericalized caption\n",
    "    caption_label = [train_dataset.vocab.itos[token] for token in cap.tolist()]\n",
    "\n",
    "    # Finding the index of '<EOS>' token to truncate the caption\n",
    "    eos_index = caption_label.index('<EOS>')\n",
    "    caption_label = caption_label[1:eos_index]\n",
    "\n",
    "    # Joining the caption label words into a single string\n",
    "    caption_label = ' '.join(caption_label)\n",
    "\n",
    "    # Create a new figure\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Displaying the image with the caption label\n",
    "    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Calculating the width of the image\n",
    "    img_width = img.shape[1]\n",
    "\n",
    "    # Calculating the height of the caption box\n",
    "    caption_height = int(img_width / 50)\n",
    "\n",
    "    # Adding a colored box with the caption label\n",
    "    plt.text(0, -10, caption_label, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round'),\n",
    "             fontsize=8, color='black', ha='left', va='top')\n",
    "\n",
    "    # Save the plot as a PDF file\n",
    "    plt.savefig(f'captionedimage{i}.pdf', format='pdf')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKTE90dPVfkn"
   },
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCLqg5fT_guf"
   },
   "source": [
    "# **Developing an Image Captioning Model:**\n",
    "- Generate data loaders for training.\n",
    "- Construct the model architecture and train the model over a specified number of epochs.\n",
    "- Assess and evaluate the model's performance through testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqyh8MPTOe8B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiH8zOVrAiJh",
    "tags": []
   },
   "source": [
    "# **1. Utilize a pre-trained ResNet model for efficient feature extraction, while fine-tuning the last linear layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoRjOLXtObJK"
   },
   "source": [
    "## **Train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0g_anB3pAlVM"
   },
   "source": [
    "### **Generate data loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAzxJ9XdVrUN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 10\n",
    "pad_idx = train_dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "# Define the transformations to be applied, including resizing, random cropping,\n",
    "# converting to tensor, and normalization using ResNet statistics\n",
    "transforms = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.RandomCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Create an instance of the FlickrDataset for training\n",
    "train_dataset = FlickrDataset(\n",
    "\n",
    "    root_dir=\"/home/jovyan/workspace/flickr_data/Images\",  # Path to the root directory of the images\n",
    "    caption_file=\"/home/jovyan/workspace/flickr_data/captions.txt\",  # Path to the captions file\n",
    "    transform=transforms,\n",
    "    data_type='train'\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the training dataset\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True,\n",
    "    collate_fn=Apppadd(pad_idx=pad_idx, batch_first=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0e8E_QqA0zC"
   },
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LigJbSoMl-qE"
   },
   "outputs": [],
   "source": [
    "#ResNet Model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, embed_size, train_resnet=False):\n",
    "\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # Load the pretrained ResNet-18 model\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Set the requires_grad flag of the ResNet parameters\n",
    "        # to control whether they are trainable or not\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(train_resnet)\n",
    "        \n",
    "        # Extract the modules of the ResNet model up to the last fully connected layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        \n",
    "        # Create the ResNet backbone with the extracted modules\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        # Create the embedding layer\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        \n",
    "        # Activation function and dropout layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "\n",
    "        features = self.dropout(self.relu(self.resnet(images)))\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "# LSTM Network\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, drop_prob=0.5):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # Word embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Linear layer for prediction\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "\n",
    "        # Vectorize the caption by passing it through the embedding layer\n",
    "        embeds = self.dropout(self.embedding(captions[:, :-1]))\n",
    "        \n",
    "        # Concatenate the features and captions\n",
    "        x = torch.cat((features.unsqueeze(1), embeds), dim=1) \n",
    "        \n",
    "        # Pass through the LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Pass through the linear layer for prediction\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def generate_caption(self, inputs, hidden=None, max_len=20, vocab=None):\n",
    "        # Inference part\n",
    "        # Given the image features, generate the captions\n",
    "        \n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        captions = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            output, hidden = self.lstm(inputs, hidden)\n",
    "            output = self.linear(output)\n",
    "            output = output.view(batch_size, -1)\n",
    "        \n",
    "            # Select the word with the highest value\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            # Save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            # End if <EOS> is detected\n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "            \n",
    "            # Send the generated word as the next caption\n",
    "            inputs = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        # Convert the vocabulary indices to words and return the sentence\n",
    "        return [vocab.itos[idx] for idx in captions]\n",
    "\n",
    "# Utilizing the powerful fusion of ResNet and LSTM for image captioning\n",
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, drop_prob=0.5, train_resnet=False):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder (ResNet)\n",
    "        self.encoder = ResNet(embed_size, train_resnet)\n",
    "        \n",
    "        # Decoder (LSTM)\n",
    "        self.decoder = LSTM(embed_size, hidden_size, vocab_size, num_layers, drop_prob)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "\n",
    "        # Pass the images through the encoder (ResNet) to get features\n",
    "        features = self.encoder(images)\n",
    "        \n",
    "        # Pass the features and captions through the decoder (LSTM) to get outputs\n",
    "        outputs = self.decoder(features, captions)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9Np4vr5FJoT"
   },
   "source": [
    "### **Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAu2T1JiWdS-",
    "outputId": "03beb12a-647c-4025-a119-a90ec83e9269"
   },
   "outputs": [],
   "source": [
    "# Check if a CUDA-enabled GPU is available\n",
    "# If available, set the device to \"cuda\" for GPU computations\n",
    "# If not available, set the device to \"cpu\" for CPU computations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "# Hyperparameters for the model\n",
    "embed_size = 128  # Size of the word embedding\n",
    "hidden_size = 256  # Size of the hidden state in the LSTM\n",
    "vocab_size = len(train_dataset.vocab)  # Size of the vocabulary\n",
    "num_layers = 1  # Number of layers in the LSTM\n",
    "learning_rate = 3e-4  # Learning rate for the optimizer\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers, train_resnet=False).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "61Eb9DzFWpNC",
    "outputId": "289988aa-9256-4842-cb62-27f4a7ea6bce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the number of training epochs\n",
    "num_epochs = 40\n",
    "\n",
    "# Create an empty list to store the training loss\n",
    "train_loss = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    \n",
    "    # Iterate over the training data loader\n",
    "    for idx, (image, captions) in enumerate(iter(train_loader)):\n",
    "        # Move the image and captions to the specified device\n",
    "        image, captions = image.to(device), captions.to(device)\n",
    "        \n",
    "        # Set the model to train mode\n",
    "        model.train()\n",
    "        \n",
    "        # Zero the gradients in the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feed forward\n",
    "        outputs = model(image, captions)\n",
    "        \n",
    "        # Calculate the batch loss\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    average_loss = running_loss / (idx + 1)\n",
    "    \n",
    "    # Print the epoch number and the average loss\n",
    "    print(f'Epoch: {epoch+1} - Train Loss: {average_loss}')\n",
    "    \n",
    "    # Append the average loss to the train_loss list\n",
    "    train_loss.append(average_loss)\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    torch.save(model.cpu().state_dict(), 'Model.pth')\n",
    "    \n",
    "    # Move the model back to the specified device\n",
    "    model.cuda()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the figure size and dpi for better quality\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Customize the grid and ticks\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.xticks(range(0, len(train_loss), 5))  # Label every 5th epoch\n",
    "plt.yticks()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"loss_plot.pdf\", format='pdf', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OaT8ZB6FyF8"
   },
   "source": [
    "## **Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm_bk6kyORhb"
   },
   "source": [
    "### **Generate test data loaders and Test the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "wbI5ht_quQDs",
    "outputId": "5cf830b1-f04b-4b55-e1dd-b9cccdf623d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_image_with_captions(image, predicted_caption, real_caption):\n",
    "    \"\"\"Display an image with predicted and real captions.\"\"\"\n",
    "    \n",
    "    # Denormalize the image tensor\n",
    "    image[0] = image[0] * 0.229\n",
    "    image[1] = image[1] * 0.224\n",
    "    image[2] = image[2] * 0.225\n",
    "    image[0] += 0.485\n",
    "    image[1] += 0.456\n",
    "    image[2] += 0.406\n",
    "    \n",
    "    # Convert the image tensor to a numpy array and transpose the dimensions\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Add predicted caption box\n",
    "    plt.text(\n",
    "        0, -20, predicted_caption, color='white', backgroundcolor='blue',\n",
    "        fontsize=12, verticalalignment='top', bbox=dict(facecolor='blue', alpha=0.8, edgecolor='white', pad=5)\n",
    "    )\n",
    "    \n",
    "    # Add real caption box\n",
    "    plt.text(\n",
    "        0, -2, real_caption, color='black', backgroundcolor='green',\n",
    "        fontsize=12, verticalalignment='top', bbox=dict(facecolor='green', alpha=0.8, edgecolor='white', pad=5)\n",
    "    )\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Define the test data transformations\n",
    "test_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),  # Resize the images to the specified size\n",
    "    T.ToTensor(),  # Convert the images to tensors\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize the image tensors\n",
    "])\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = FlickrDataset(\n",
    "    root_dir=\"/home/jovyan/workspace/flickr_data/Images\",  # Path to the root directory of the images\n",
    "    caption_file=\"/home/jovyan/workspace/flickr_data/captions.txt\",  # Path to the captions file\n",
    "    transform=test_transforms,  # Apply the specified transformations to the images\n",
    "    frequency_threshold=1,  # Frequency threshold for filtering captions\n",
    "    data_type='test'  # Specify the type of data (in this case, 'test')\n",
    ")\n",
    "\n",
    "# Create the test data loader\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,  # Use the created test dataset\n",
    "    batch_size=BATCH_SIZE,  # Number of samples per batch\n",
    "    num_workers=NUM_WORKERS,  # Number of worker threads for data loading\n",
    "    shuffle=True,  # Shuffle the data for each epoch\n",
    "    collate_fn=Apppadd(pad_idx=pad_idx, batch_first=True)  # Function to collate and preprocess the data\n",
    ")\n",
    "\n",
    "def check_test_image():\n",
    "    # Get a batch of images and captions from the test loader\n",
    "    images, captions = next(iter(test_loader))\n",
    "  \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get a single test image and create a clone\n",
    "    test_image = torch.clone(images)[0].unsqueeze(0)\n",
    "    \n",
    "    # Disable gradient calculation during inference\n",
    "    with torch.no_grad():\n",
    "        # Encode the test image using the model's encoder\n",
    "        features = model.encoder(test_image[0:1].to(device))\n",
    "        \n",
    "        # Generate captions for the test image using the model's decoder\n",
    "        predicted_captions = model.decoder.generate_caption(features.unsqueeze(0), vocab=train_dataset.vocab)\n",
    "        \n",
    "        # Get the ground truth caption for the test image\n",
    "        cap = torch.clone(captions)[0]\n",
    "        caption_label = [test_dataset.vocab.itos[token] for token in cap.tolist()]\n",
    "        \n",
    "        # Find the index of the end-of-sequence token '<EOS>' in the caption\n",
    "        eos_index = caption_label.index('<EOS>')\n",
    "        \n",
    "        # Extract the caption tokens from the start to the '<EOS>' token\n",
    "        caption_label = caption_label[1:eos_index]\n",
    "        \n",
    "        # Convert the caption tokens to a string\n",
    "        caption_label = ' '.join(caption_label)\n",
    "        \n",
    "        # Create the predicted caption string with proper formatting\n",
    "        predicted_caption = \"Predicted Caption: \" + ' '.join(predicted_captions[1:len(predicted_captions)-1])\n",
    "        \n",
    "        # Create the actual caption string with proper formatting\n",
    "        real_caption = \"Actual Caption: \" + caption_label\n",
    "        \n",
    "        # Display the image with the predicted and real captions\n",
    "        show_image_with_captions(test_image[0], predicted_caption, real_caption)\n",
    "    # Save the plot as a PDF file (you can use a different filename if needed)\n",
    "    plt.savefig(f\"testimagecaptions_1.pdf\", format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "hG7dDM2ajPw3",
    "outputId": "c3716f58-11e3-4825-a0df-1b0d00de6d98"
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "d1UXMYZ7jZPj",
    "outputId": "2889c7c6-3ff2-47e5-df44-7bb768e76014"
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "nRkaBqHRjgkO",
    "outputId": "4c6725bd-f853-4127-c117-446688183408"
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "rv6OZdgdjsld",
    "outputId": "75d5cb3b-4a74-4690-e4fb-eeb0d21732d5"
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "DvDb-YQsOrIp",
    "outputId": "4171186e-79b3-4bd5-d74f-d1046b05b612"
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbBV_fPUbG3Y",
    "tags": []
   },
   "source": [
    "# **2. Utilize a ResNet model (without freezing) for efficient feature extraction, while fine-tuning the last linear layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYEx_lBbbQ08"
   },
   "source": [
    "## **Train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrUoPvJQbQ08",
    "tags": []
   },
   "source": [
    "### **Generate data loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBc5jFDAbQ08",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 10\n",
    "pad_idx = train_dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "# Define the transformations to be applied, including resizing, random cropping,\n",
    "# converting to tensor, and normalization using ResNet statistics\n",
    "transforms = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.RandomCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Create an instance of the FlickrDataset for training\n",
    "train_dataset = FlickrDataset(\n",
    "    root_dir=\"/home/jovyan/workspace/flickr_data/Images\",  # Path to the root directory of the images\n",
    "    caption_file=\"/home/jovyan/workspace/flickr_data/captions.txt\",  # Path to the captions file\n",
    "    transform=transforms,\n",
    "    data_type='train'\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the training dataset\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True,\n",
    "    collate_fn=Apppadd(pad_idx=pad_idx, batch_first=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqZ3MIklbQ08"
   },
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swHXSazzbQ08",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ResNet Model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, embed_size, train_resnet=False):\n",
    "        \"\"\"\n",
    "        Initialize the ResNet model with a specified embedding size.\n",
    "        \n",
    "        Args:\n",
    "            embed_size (int): Size of the embedding output.\n",
    "            train_resnet (bool): Whether to train the ResNet backbone or not.\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # Load the pretrained ResNet-18 model\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Set the requires_grad flag of the ResNet parameters\n",
    "        # to control whether they are trainable or not\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(train_resnet)\n",
    "        \n",
    "        # Extract the modules of the ResNet model up to the last fully connected layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        \n",
    "        # Create the ResNet backbone with the extracted modules\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        # Create the embedding layer\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        \n",
    "        # Activation function and dropout layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass of the ResNet model.\n",
    "        \n",
    "        Args:\n",
    "            images (tensor): Input images tensor.\n",
    "        \n",
    "        Returns:\n",
    "            features (tensor): Embedded features tensor.\n",
    "        \"\"\"\n",
    "        features = self.dropout(self.relu(self.resnet(images)))\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "# LSTM Network\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model with specified sizes and parameters.\n",
    "        \n",
    "        Args:\n",
    "            embed_size (int): Size of the word embedding.\n",
    "            hidden_size (int): Size of the hidden state of the LSTM.\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            num_layers (int): Number of layers in the LSTM (default: 1).\n",
    "            drop_prob (float): Dropout probability (default: 0.5).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Word embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Linear layer for prediction\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM model.\n",
    "        \n",
    "        Args:\n",
    "            features (tensor): Image features tensor.\n",
    "            captions (tensor): Captions tensor.\n",
    "        \n",
    "        Returns:\n",
    "            x (tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        # Vectorize the caption by passing it through the embedding layer\n",
    "        embeds = self.dropout(self.embedding(captions[:, :-1]))\n",
    "        \n",
    "        # Concatenate the features and captions\n",
    "        x = torch.cat((features.unsqueeze(1), embeds), dim=1) \n",
    "        \n",
    "        # Pass through the LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Pass through the linear layer for prediction\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def generate_caption(self, inputs, hidden=None, max_len=20, vocab=None):\n",
    "        \"\"\"\n",
    "        Generate captions given the image features.\n",
    "        \n",
    "        Args:\n",
    "            inputs (tensor): Input tensor of image features.\n",
    "            hidden (tuple): Hidden state of the LSTM (default: None).\n",
    "            max_len (int): Maximum length of the generated caption (default: 20).\n",
    "            vocab (Vocab): Vocabulary object (default: None).\n",
    "        \n",
    "        Returns:\n",
    "            caption (list): Generated caption as a list of words.\n",
    "        \"\"\"\n",
    "        # Inference part\n",
    "        # Given the image features, generate the captions\n",
    "        \n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        captions = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            output, hidden = self.lstm(inputs, hidden)\n",
    "            output = self.linear(output)\n",
    "            output = output.view(batch_size, -1)\n",
    "        \n",
    "            # Select the word with the highest value\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            # Save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            # End if <EOS> is detected\n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "            \n",
    "            # Send the generated word as the next caption\n",
    "            inputs = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        # Convert the vocabulary indices to words and return the sentence\n",
    "        return [vocab.itos[idx] for idx in captions]\n",
    "\n",
    "# Utilizing the powerful fusion of ResNet and LSTM for image captioning\n",
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, drop_prob=0.5, train_resnet=False):\n",
    "        \"\"\"\n",
    "        Initialize the CNNtoRNN model with specified sizes and parameters.\n",
    "        \n",
    "        Args:\n",
    "            embed_size (int): Size of the word embedding.\n",
    "            hidden_size (int): Size of the hidden state of the LSTM.\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            num_layers (int): Number of layers in the LSTM (default: 1).\n",
    "            drop_prob (float): Dropout probability (default: 0.5).\n",
    "            train_resnet (bool): Whether to train the ResNet backbone or not (default: False).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder (ResNet)\n",
    "        self.encoder = ResNet(embed_size, train_resnet)\n",
    "        \n",
    "        # Decoder (LSTM)\n",
    "        self.decoder = LSTM(embed_size, hidden_size, vocab_size, num_layers, drop_prob)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        \"\"\"\n",
    "        Forward pass of the CNNtoRNN model.\n",
    "        \n",
    "        Args:\n",
    "            images (tensor): Input images tensor.\n",
    "            captions (tensor): Captions tensor.\n",
    "        \n",
    "        Returns:\n",
    "            outputs (tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        # Pass the images through the encoder (ResNet) to get features\n",
    "        features = self.encoder(images)\n",
    "        \n",
    "        # Pass the features and captions through the decoder (LSTM) to get outputs\n",
    "        outputs = self.decoder(features, captions)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puxAS0zgbQ09"
   },
   "source": [
    "### **Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFLsb-D2bQ09",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if a CUDA-enabled GPU is available\n",
    "# If available, set the device to \"cuda\" for GPU computations\n",
    "# If not available, set the device to \"cpu\" for CPU computations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "# Hyperparameters for the model\n",
    "embed_size = 128  # Size of the word embedding\n",
    "hidden_size = 256  # Size of the hidden state in the LSTM\n",
    "vocab_size = len(train_dataset.vocab)  # Size of the vocabulary\n",
    "num_layers = 1  # Number of layers in the LSTM\n",
    "learning_rate = 3e-4  # Learning rate for the optimizer\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "NoFreeze_model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers, train_resnet=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(NoFreeze_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4CRBPlYQbQ09",
    "outputId": "d8205985-5b62-4d08-f23c-5c19d4f4cde5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the number of training epochs\n",
    "num_epochs = 40\n",
    "\n",
    "# Create an empty list to store the training loss\n",
    "NoFreeze_train_loss = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    \n",
    "    # Iterate over the training data loader\n",
    "    for idx, (image, captions) in enumerate(iter(train_loader)):\n",
    "        # Move the image and captions to the specified device\n",
    "        image, captions = image.to(device), captions.to(device)\n",
    "        \n",
    "        # Set the model to train mode\n",
    "        NoFreeze_model.train()\n",
    "        \n",
    "        # Zero the gradients in the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feed forward\n",
    "        outputs = NoFreeze_model(image, captions)\n",
    "        \n",
    "        # Calculate the batch loss\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    average_loss = running_loss / (idx + 1)\n",
    "    \n",
    "    # Print the epoch number and the average loss\n",
    "    print(f'Epoch: {epoch+1} - Train Loss: {average_loss}')\n",
    "    \n",
    "    # Append the average loss to the train_loss list\n",
    "    NoFreeze_train_loss.append(average_loss)\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    torch.save(NoFreeze_model.cpu().state_dict(), 'NoFreezeModel.pth')\n",
    "    \n",
    "    # Move the model back to the specified device\n",
    "    NoFreeze_model.cuda()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the figure size and dpi for better quality\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(NoFreeze_train_loss, label='Training Loss')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Customize the grid and ticks\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.xticks(range(0, len(NoFreeze_train_loss), 5))  # Label every 5th epoch\n",
    "plt.yticks()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"loss_plot2.pdf\", format='pdf', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "1caP35oTz9nd",
    "outputId": "c4c23069-05a8-4d62-8e3a-dfc2f3f139a9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Plot the training loss curves\n",
    "plt.plot(train_loss, label='Without Fine-tuning (Freeze)')\n",
    "plt.plot(NoFreeze_train_loss, label='With Fine-tuning (No Freeze)')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Add grid lines to the plot\n",
    "plt.grid(True)\n",
    "\n",
    "# Adjust the layout for a more compact and professional appearance\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"comparelossplot2.pdf\", format='pdf', bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NoFreeze_model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers, train_resnet=True).to(device)\n",
    "\n",
    "# # Load the model's state dictionary\n",
    "# NoFreeze_model.load_state_dict(torch.load('NoFreezeModel.pth'))\n",
    "NoFreeze_model = torch.load('/home/jovyan/workspace/NoFreezeModel.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mW6z3wEabQ0-"
   },
   "source": [
    "## **Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVriUdAzbQ0-"
   },
   "source": [
    "### **Generate test data loaders and Test the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "UZekpCEpbQ0-",
    "outputId": "dc2bccea-2720-4fb2-b9fd-469d0ad7830b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_image_with_captions(image, predicted_caption, real_caption):\n",
    "    \"\"\"Display an image with predicted and real captions.\"\"\"\n",
    "    \n",
    "    # Denormalize the image tensor\n",
    "    image[0] = image[0] * 0.229\n",
    "    image[1] = image[1] * 0.224\n",
    "    image[2] = image[2] * 0.225\n",
    "    image[0] += 0.485\n",
    "    image[1] += 0.456\n",
    "    image[2] += 0.406\n",
    "    \n",
    "    # Convert the image tensor to a numpy array and transpose the dimensions\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Add predicted caption box\n",
    "    plt.text(\n",
    "        0, -20, predicted_caption, color='white', backgroundcolor='blue',\n",
    "        fontsize=12, verticalalignment='top', bbox=dict(facecolor='blue', alpha=0.8, edgecolor='white', pad=5)\n",
    "    )\n",
    "    \n",
    "    # Add real caption box\n",
    "    plt.text(\n",
    "        0, -2, real_caption, color='black', backgroundcolor='green',\n",
    "        fontsize=12, verticalalignment='top', bbox=dict(facecolor='green', alpha=0.8, edgecolor='white', pad=5)\n",
    "    )\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Define the test data transformations\n",
    "test_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),  # Resize the images to the specified size\n",
    "    T.ToTensor(),  # Convert the images to tensors\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize the image tensors\n",
    "])\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = FlickrDataset(\n",
    "    root_dir=\"/home/jovyan/workspace/flickr_data/Images\",  # Path to the root directory of the images\n",
    "    caption_file=\"/home/jovyan/workspace/flickr_data/captions.txt\",  # Path to the captions file\n",
    "    transform=test_transforms,  # Apply the specified transformations to the images\n",
    "    frequency_threshold=1,  # Frequency threshold for filtering captions\n",
    "    data_type='test'  # Specify the type of data (in this case, 'test')\n",
    ")\n",
    "\n",
    "# Create the test data loader\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,  # Use the created test dataset\n",
    "    batch_size=BATCH_SIZE,  # Number of samples per batch\n",
    "    num_workers=NUM_WORKERS,  # Number of worker threads for data loading\n",
    "    shuffle=True,  # Shuffle the data for each epoch\n",
    "    collate_fn=Apppadd(pad_idx=pad_idx, batch_first=True)  # Function to collate and preprocess the data\n",
    ")\n",
    "# Run the code 10 times\n",
    "def check_test_image():\n",
    "    # Get a batch of images and captions from the test loader\n",
    "    images, captions = next(iter(test_loader))\n",
    "  \n",
    "    # Set the model to evaluation mode\n",
    "    NoFreezeModel.eval()\n",
    "\n",
    "    # Get a single test image and create a clone\n",
    "    test_image = torch.clone(images)[0].unsqueeze(0)\n",
    "    \n",
    "    # Disable gradient calculation during inference\n",
    "    with torch.no_grad():\n",
    "        # Encode the test image using the model's encoder\n",
    "        features = NoFreeze_model.encoder(test_image[0:1].to(device))\n",
    "        \n",
    "        # Generate captions for the test image using the model's decoder\n",
    "        predicted_captions = NoFreezeModel.decoder.generate_caption(features.unsqueeze(0), vocab=train_dataset.vocab)\n",
    "        \n",
    "        # Get the ground truth caption for the test image\n",
    "        cap = torch.clone(captions)[0]\n",
    "        caption_label = [test_dataset.vocab.itos[token] for token in cap.tolist()]\n",
    "        \n",
    "        # Find the index of the end-of-sequence token '<EOS>' in the caption\n",
    "        eos_index = caption_label.index('<EOS>')\n",
    "        \n",
    "        # Extract the caption tokens from the start to the '<EOS>' token\n",
    "        caption_label = caption_label[1:eos_index]\n",
    "        \n",
    "        # Convert the caption tokens to a string\n",
    "        caption_label = ' '.join(caption_label)\n",
    "        \n",
    "        # Create the predicted caption string with proper formatting\n",
    "        predicted_caption = \"Predicted Caption: \" + ' '.join(predicted_captions[1:len(predicted_captions)-1])\n",
    "        \n",
    "        # Create the actual caption string with proper formatting\n",
    "        real_caption = \"Actual Caption: \" + caption_label\n",
    "        \n",
    "        # Display the image with the predicted and real captions\n",
    "        show_image_with_captions(test_image[0], predicted_caption, real_caption)\n",
    "    # Save the plot as a PDF file (you can use a different filename if needed)\n",
    "    plt.savefig(f\"testimagecaptions_1.pdf\", format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "hjo7qnY5b_mP",
    "outputId": "4a50a985-1286-4426-c05a-50b232c416e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "Y-B2A0MgcAp5",
    "outputId": "28196c0b-7048-4ff3-bf3e-874a7e9af435"
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "9EU56i3039BA",
    "outputId": "8c68b17b-91ec-4d79-e826-8a466d7f25b1"
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "aeXF9pnN3-bW",
    "outputId": "611a196c-948c-4c7e-f3a3-1a5d00f4e202",
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "cYMi7EZ44ouT",
    "outputId": "b621f3b9-2f66-46ff-a355-825e1feceecb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CK7DBh32dyP2"
   },
   "source": [
    "# **3. Utilize a ResNet model (without freezing + Bidirectional LSTM) for efficient feature extraction, while fine-tuning the last linear layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2T8VWeZdyP3"
   },
   "source": [
    "## **Train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqkRuXFBdyP3"
   },
   "source": [
    "### **Generate data loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jo6a_uWzdyP3"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 10\n",
    "pad_idx = train_dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "# Define the transformations to be applied, including resizing, random cropping,\n",
    "# converting to tensor, and normalization using ResNet statistics\n",
    "transforms = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.RandomCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Create an instance of the FlickrDataset for training\n",
    "train_dataset = FlickrDataset(\n",
    "    root_dir=\"/home/jovyan/workspace/flickr_data/Images\",  # Path to the root directory of the images\n",
    "    caption_file=\"/home/jovyan/workspace/flickr_data/captions.txt\",  # Path to the captions file\n",
    "    transform=transforms,\n",
    "    data_type='train'\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the training dataset\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True,\n",
    "    collate_fn=Apppadd(pad_idx=pad_idx, batch_first=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDRH9hjYdyP3"
   },
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFudwR3tdyP3"
   },
   "outputs": [],
   "source": [
    "#ResNet Model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, embed_size, train_resnet=False):\n",
    "        \"\"\"\n",
    "        Initialize the ResNet model with a specified embedding size.\n",
    "        \n",
    "        Args:\n",
    "            embed_size (int): Size of the embedding output.\n",
    "            train_resnet (bool): Whether to train the ResNet backbone or not.\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # Load the pretrained ResNet-18 model\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Set the requires_grad flag of the ResNet parameters\n",
    "        # to control whether they are trainable or not\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(train_resnet)\n",
    "        \n",
    "        # Extract the modules of the ResNet model up to the last fully connected layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        \n",
    "        # Create the ResNet backbone with the extracted modules\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        # Create the embedding layer\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        \n",
    "        # Activation function and dropout layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass of the ResNet model.\n",
    "        \n",
    "        Args:\n",
    "            images (tensor): Input images tensor.\n",
    "        \n",
    "        Returns:\n",
    "            features (tensor): Embedded features tensor.\n",
    "        \"\"\"\n",
    "        features = self.dropout(self.relu(self.resnet(images)))\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "# LSTM Network\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model with specified sizes and parameters.\n",
    "        \n",
    "        Args:\n",
    "            embed_size (int): Size of the word embedding.\n",
    "            hidden_size (int): Size of the hidden state of the LSTM.\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            num_layers (int): Number of layers in the LSTM (default: 1).\n",
    "            drop_prob (float): Dropout probability (default: 0.5).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Word embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional = True)\n",
    "        \n",
    "        # Linear layer for prediction\n",
    "        self.linear = nn.Linear(hidden_size*2, vocab_size)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM model.\n",
    "        \n",
    "        Args:\n",
    "            features (tensor): Image features tensor.\n",
    "            captions (tensor): Captions tensor.\n",
    "        \n",
    "        Returns:\n",
    "            x (tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        # Vectorize the caption by passing it through the embedding layer\n",
    "        embeds = self.dropout(self.embedding(captions[:, :-1]))\n",
    "        \n",
    "        # Concatenate the features and captions\n",
    "        x = torch.cat((features.unsqueeze(1), embeds), dim=1) \n",
    "        \n",
    "        # Pass through the LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Pass through the linear layer for prediction\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def generate_caption(self, inputs, hidden=None, max_len=20, vocab=None):\n",
    "        \"\"\"\n",
    "        Generate captions given the image features.\n",
    "        \n",
    "        Args:\n",
    "            inputs (tensor): Input tensor of image features.\n",
    "            hidden (tuple): Hidden state of the LSTM (default: None).\n",
    "            max_len (int): Maximum length of the generated caption (default: 20).\n",
    "            vocab (Vocab): Vocabulary object (default: None).\n",
    "        \n",
    "        Returns:\n",
    "            caption (list): Generated caption as a list of words.\n",
    "        \"\"\"\n",
    "        # Inference part\n",
    "        # Given the image features, generate the captions\n",
    "        \n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        captions = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            output, hidden = self.lstm(inputs, hidden)\n",
    "            output = self.linear(output)\n",
    "            output = output.view(batch_size, -1)\n",
    "        \n",
    "            # Select the word with the highest value\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            # Save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            # End if <EOS> is detected\n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "            \n",
    "            # Send the generated word as the next caption\n",
    "            inputs = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        # Convert the vocabulary indices to words and return the sentence\n",
    "        return [vocab.itos[idx] for idx in captions]\n",
    "\n",
    "# Utilizing the powerful fusion of ResNet and LSTM for image captioning\n",
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, drop_prob=0.5, train_resnet=False):\n",
    "        \"\"\"\n",
    "        Initialize the CNNtoRNN model with specified sizes and parameters.\n",
    "        \n",
    "        Args:\n",
    "            embed_size (int): Size of the word embedding.\n",
    "            hidden_size (int): Size of the hidden state of the LSTM.\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            num_layers (int): Number of layers in the LSTM (default: 1).\n",
    "            drop_prob (float): Dropout probability (default: 0.5).\n",
    "            train_resnet (bool): Whether to train the ResNet backbone or not (default: False).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder (ResNet)\n",
    "        self.encoder = ResNet(embed_size, train_resnet)\n",
    "        \n",
    "        # Decoder (LSTM)\n",
    "        self.decoder = LSTM(embed_size, hidden_size, vocab_size, num_layers, drop_prob)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        \"\"\"\n",
    "        Forward pass of the CNNtoRNN model.\n",
    "        \n",
    "        Args:\n",
    "            images (tensor): Input images tensor.\n",
    "            captions (tensor): Captions tensor.\n",
    "        \n",
    "        Returns:\n",
    "            outputs (tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        # Pass the images through the encoder (ResNet) to get features\n",
    "        features = self.encoder(images)\n",
    "        \n",
    "        # Pass the features and captions through the decoder (LSTM) to get outputs\n",
    "        outputs = self.decoder(features, captions)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vjpxih4WdyP4"
   },
   "source": [
    "### **Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmAb_ZICdyP4"
   },
   "outputs": [],
   "source": [
    "# Check if a CUDA-enabled GPU is available\n",
    "# If available, set the device to \"cuda\" for GPU computations\n",
    "# If not available, set the device to \"cpu\" for CPU computations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "# Hyperparameters for the model\n",
    "embed_size = 128  # Size of the word embedding\n",
    "hidden_size = 256  # Size of the hidden state in the LSTM\n",
    "vocab_size = len(train_dataset.vocab)  # Size of the vocabulary\n",
    "num_layers = 1  # Number of layers in the LSTM\n",
    "learning_rate = 3e-4  # Learning rate for the optimizer\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "NoFreeze_model_bi = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers, train_resnet=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(NoFreeze_model_bi.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "id": "1iDNSfwpdyP4",
    "outputId": "a526ad22-57e8-4431-d9f3-4e31418353c3"
   },
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Set the number of training epochs\n",
    "num_epochs = 40\n",
    "\n",
    "# Create an empty list to store the training loss\n",
    "NoFreeze_train_loss_bi = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    \n",
    "    # Iterate over the training data loader\n",
    "    for idx, (image, captions) in enumerate(iter(train_loader)):\n",
    "        # Move the image and captions to the specified device\n",
    "        image, captions = image.to(device), captions.to(device)\n",
    "        \n",
    "        # Set the model to train mode\n",
    "        NoFreeze_model_bi.train()\n",
    "        \n",
    "        # Zero the gradients in the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feed forward\n",
    "        outputs = NoFreeze_model_bi(image, captions)\n",
    "        \n",
    "        # Calculate the batch loss\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer\n",
    "        scaler.step(optimizer) \n",
    "        scaler.update() \n",
    "        \n",
    "        # Accumulate the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    average_loss = running_loss / (idx + 1)\n",
    "    \n",
    "    # Print the epoch number and the average loss\n",
    "    print(f'Epoch: {epoch+1} - Train Loss: {average_loss}')\n",
    "    \n",
    "    # Append the average loss to the train_loss list\n",
    "    NoFreeze_train_loss_bi.append(average_loss)\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    torch.save(NoFreeze_model_bi.cpu().state_dict(), 'NoFreezeModelbi.pth')\n",
    "    \n",
    "    # Move the model back to the specified device\n",
    "    NoFreeze_model_bi.cuda()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the figure size and dpi for better quality\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(NoFreeze_train_loss_bi, label='Training Loss')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Customize the grid and ticks\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.xticks(range(0, len(NoFreeze_train_loss_bi), 5))  # Label every 5th epoch\n",
    "plt.yticks()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"lossplot3.pdf\", format='pdf', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "RDiK_EZi7UV3",
    "outputId": "8dc85f89-1863-4d96-8553-a70991f15df7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the figure size and dpi for better quality\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(NoFreeze_train_loss_bi, label='Training Loss')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Customize the grid and ticks\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.xticks(range(0, len(NoFreeze_train_loss_bi), 5))  # Label every 5th epoch\n",
    "plt.yticks()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"lossplot3.pdf\", format='pdf', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovZnhED4dyP5"
   },
   "source": [
    "## **Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0B-G8A-dyP5"
   },
   "source": [
    "### **Generate test data loaders and Test the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "4ZjR3hkWdyP5",
    "outputId": "0578ffc6-a821-4060-bab0-69f4c8a61756"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_image_with_captions(image, predicted_caption, real_caption):\n",
    "    \"\"\"Display an image with predicted and real captions.\"\"\"\n",
    "    \n",
    "    # Denormalize the image tensor\n",
    "    image[0] = image[0] * 0.229\n",
    "    image[1] = image[1] * 0.224\n",
    "    image[2] = image[2] * 0.225\n",
    "    image[0] += 0.485\n",
    "    image[1] += 0.456\n",
    "    image[2] += 0.406\n",
    "    \n",
    "    # Convert the image tensor to a numpy array and transpose the dimensions\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Add predicted caption box\n",
    "    plt.text(\n",
    "        0, -20, predicted_caption, color='white', backgroundcolor='blue',\n",
    "        fontsize=12, verticalalignment='top', bbox=dict(facecolor='blue', alpha=0.8, edgecolor='white', pad=5)\n",
    "    )\n",
    "    \n",
    "    # Add real caption box\n",
    "    plt.text(\n",
    "        0, -2, real_caption, color='black', backgroundcolor='green',\n",
    "        fontsize=12, verticalalignment='top', bbox=dict(facecolor='green', alpha=0.8, edgecolor='white', pad=5)\n",
    "    )\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Define the test data transformations\n",
    "test_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),  # Resize the images to the specified size\n",
    "    T.ToTensor(),  # Convert the images to tensors\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize the image tensors\n",
    "])\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = FlickrDataset(\n",
    "    root_dir=\"/home/jovyan/workspace/flickr_data/Images\",  # Path to the root directory of the images\n",
    "    caption_file=\"/home/jovyan/workspace/flickr_data/captions.txt\",  # Path to the captions file\n",
    "    transform=test_transforms,  # Apply the specified transformations to the images\n",
    "    frequency_threshold=1,  # Frequency threshold for filtering captions\n",
    "    data_type='test'  # Specify the type of data (in this case, 'test')\n",
    ")\n",
    "\n",
    "# Create the test data loader\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,  # Use the created test dataset\n",
    "    batch_size=BATCH_SIZE,  # Number of samples per batch\n",
    "    num_workers=NUM_WORKERS,  # Number of worker threads for data loading\n",
    "    shuffle=True,  # Shuffle the data for each epoch\n",
    "    collate_fn=Apppadd(pad_idx=pad_idx, batch_first=True)  # Function to collate and preprocess the data\n",
    ")\n",
    "# Run the code 10 times\n",
    "def check_test_image():\n",
    "    # Get a batch of images and captions from the test loader\n",
    "    images, captions = next(iter(test_loader))\n",
    "  \n",
    "    # Set the model to evaluation mode\n",
    "    NoFreeze_model_bi.eval()\n",
    "\n",
    "    # Get a single test image and create a clone\n",
    "    test_image = torch.clone(images)[0].unsqueeze(0)\n",
    "    \n",
    "    # Disable gradient calculation during inference\n",
    "    with torch.no_grad():\n",
    "        # Encode the test image using the model's encoder\n",
    "        features = NoFreeze_model_bi.encoder(test_image[0:1].to(device))\n",
    "        \n",
    "        # Generate captions for the test image using the model's decoder\n",
    "        predicted_captions = NoFreeze_model.decoder.generate_caption(features.unsqueeze(0), vocab=train_dataset.vocab)\n",
    "        \n",
    "        # Get the ground truth caption for the test image\n",
    "        cap = torch.clone(captions)[0]\n",
    "        caption_label = [test_dataset.vocab.itos[token] for token in cap.tolist()]\n",
    "        \n",
    "        # Find the index of the end-of-sequence token '<EOS>' in the caption\n",
    "        eos_index = caption_label.index('<EOS>')\n",
    "        \n",
    "        # Extract the caption tokens from the start to the '<EOS>' token\n",
    "        caption_label = caption_label[1:eos_index]\n",
    "        \n",
    "        # Convert the caption tokens to a string\n",
    "        caption_label = ' '.join(caption_label)\n",
    "        \n",
    "        # Create the predicted caption string with proper formatting\n",
    "        predicted_caption = \"Predicted Caption: \" + ' '.join(predicted_captions[1:len(predicted_captions)-1])\n",
    "        \n",
    "        # Create the actual caption string with proper formatting\n",
    "        real_caption = \"Actual Caption: \" + caption_label\n",
    "        \n",
    "        # Display the image with the predicted and real captions\n",
    "        show_image_with_captions(test_image[0], predicted_caption, real_caption)\n",
    "    # Save the plot as a PDF file (you can use a different filename if needed)\n",
    "    plt.savefig(f\"testimagecaptions4.pdf\", format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "PnAn6RwpdyP5",
    "outputId": "936c9d67-c231-4007-98cb-af2572f5897a"
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "Qq9lzB4tdyP6",
    "outputId": "5614320b-c886-496f-efcc-b6b40fe0fe92",
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_test_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
